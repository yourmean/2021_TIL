# 오버피팅

Overfitting(과적합)이란 무엇일까요? 말 그대로 어떤 학습 데이터에 모델이 과하게 학습되어 새로운 데이터에서 제대로 동작하지 않는 현상을 말합니다. 그럼 이 Overfitting(과적합)을 막을 수 있는 방법에는 뭐가 있을까요?

오늘은 크게 4가지를 다뤄보려고 합니다!

- 데이터 조절
  과적합의 경우 데이터의 양이 적은 경우에도 발생 가능합니다. 해당 데이터의 노이즈, 패턴 등등을 쉽게 암기해 버리기 때문이죠! 그렇기 때문에 데이터의 양을 늘릴수록 데이터의 일반적인 패턴을 학습하는 과적합을 방지할 수 있겠죠. 이를 위해 의도적으로 데이터 양을 늘리는 Data Augmentation 기법을 활용하기도 합니다. (이미지로 예를 들면, 이미지를 회전하거나 노이즈를 주거나 일부를 자르거나 하는등의 방법을 말합니다.) 
  또한, 앞서 논의 하였던 Cross Validation을 활용하기도 하죠. validation data set을 활용하는 것이 왜 Overfitting(과적합)을 해결해 줄까요? 이 질문에 대한 답은 해결보다는 확인해서 수정할 수 있다 라는 표현이 더 맞을 것 같네요. validation data를 활용해 중간중간 성능을 확인하고 성능저하가 심하다면 모델을 수정하여 Overfitting(과적합)을 방지하는 것이죠.

- 모델 복잡도 감소
  두번째로는 모델의 복잡도를 감소 시키는 것입니다. 모델을 무조건 복잡하게 설계한다고 해서 성능이 오르는 것은 아닙니다.  이 경우 오히려 특정 데이터에 대해 조절해나가서 Overfitting(과적합)을 초래하기도 합니다. 따라서 Overfitting(과적합)이 발생했을 때, 모델의 복잡도를 내려보는 것도 하나의 방법이 될 수 있습니다.

- Regularization 적용
  세번째로 Regularization을 적용하는 겁니다. 앞서 언급했듯 복잡한 모델이 간단한 모델 보다 Overfitting(과적합)될 가능성이 높습니다. 그렇다면 복잡한 모델은 무엇일까요? hidden layer 수, 매개변수의 수 등등에 따라 모델의 복잡도가 결정됩니다. 이런 변수들이 많아 지면 "The Curs of Dimensionality"(차원의 저주)에 빠지기도하죠. 복잡한 모델을 좀 간단하게 만드는 방법 중 하나가 Regularization입니다. (이를 규제, 정규화, 일반화 등 다양하게 번역하지만 Nomalization과 혼동될 수 있으니 그냥 Regularization이라고만 표기하겠습니다.) Regularization은 모든 feature들을 활용하지만, 파라미터의 값과 크기를 줄여서 해결하는 방식입니다. 
  이 Regularization을 직관적인 언어로 설명하면, 어떤 데이터에 과하게 학습되어 함수가 4차방정식으로 이루어졌다고 해봅시다. 이때 일부 파라미터를 작은 값으로 만들면 함수가 간단해 지겠죠? 예를 들면 이겁니다. H(x)=w1x1+w2x2+w3x3+w4x4 라는 수식이 있을때, Regularization을 적용했더니 w3이 0이 되었다고 해봅시다. 그렇다면, x3은 모델의 결과에 영향을 별로 주지 않는다는 것을 뜻하게 되죠. 
  이때, Regularization은 L1, L2 두가지를 간단히 설명하면,
  1. L1은 비용함수가 최소가 되게하는 가중치(w)와 bias를 찾는 것
  2. L2는 가중치들의 제곱을 최소화하여 가중치(w)를 0에 가까워지게 하는 것 입니다.
  L1 Regularization는 어떤 파라미터가 모델에 영향을 주고싶은지 판단하고 싶을 때 주로 사용되며, 이런 경우가 아닌 경우에는 L2 Regularization를 일반적으로 더 많이 사용합니다.

- Dropout 적용
  자 마지막으로 Dropout입니다. Dropout은 학습과정에서 일부 신경망을 사용하지 않는 방법이죠. 예를 들어 Dropout비율이 0.5라면 학습과정마다 랜덤으로 반절의 뉴런만 사용하는 것입니다. Dropout 같은 경우, 신경망 학습시에만 사용하고 prediction시에는 사용하지 않는 것이 일반적입니다. 학습시 특정 뉴런이나 조합에 의존적으로 학습되는 것을 방지하여 Overfitting(과적합)을 막아주는 것이죠. 또한 주어진 비율에 따라 랜덤하게 뉴런을 선택하여 학습하기 때문에 앙상블 효과를 주기도합니다.
