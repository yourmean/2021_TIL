# Exploitation vs Exploration

강화 학습의 용어를 remind 해보면, State, Action, Reward, Observation, Policy, Agent 등이 있었습니다. Agent가 어떤 State에서 할 수 있는 최고의 행동을 찾는 게 목적인 것이죠. 이 때 "어떤 행동을 할까?" 에 대한 질문에 대한 대답을 Policy가 대답해줍니다!

Policy가 대답하는 패턴은 Training 일 때와 Validating 일 때에 따라 다릅니다. 먼저 학습이 끝난 시점의 Policy 라면? 자기가 알고 있는 한 가장 좋은 Action을 돌려주면 되겠죠. 즉, Action_policy(S) = Argmax(Return of Action(S, A)) 가 될 것입니다. 여기서 Return 이란 용어는, 단순한 Action의 immediate reward가 아니라, 그 이후의 먼 미래까지의 Reward의 총 누적값입니다. 예를 들면 바둑으로 쳤을 때 중간 중간 돌을 놓는 행위는 immediate하게 승패를 결정하지 못하기 때문에 reward가 0 이지만, 먼 미래에 승리할 확률이 return 으로 계산될 것입니다.

Validation 일 때는 당연하게도 제일 좋은 방향으로 가겠지만, Training 중에서는 어떨까요? 강화 학습 또한 Policy 라는 Function을 Optimization하는 과정입니다. 하지만 어려운 문제로 갈수록(Continuous State, Action domain / Model Free / Partially Observable 등) 문제 공간의 복잡도가 엄청 복잡해져서 local maximum에 빠지게 되는 일이 허다하게 발생합니다.

그럼 여기서 던질 수 있는 질문은 "그럼 어떻게 학습해야 local maximum에 빠지지 않을까?" 겠죠. 여기서 등장하는 용어가 Exploitation vs Exploration 입니다. "내가 알던 행동으로 진행해볼까?" vs "아냐 이번엔 좀 새로운 방향으로 가보자! 혹시 몰라? 더 좋은 결과가 될 지?" 의 차이라고 보시면 됩니다.

Function optimization 측면에서 바라보면 "Toward local maximum" vs "Escape local maximum" 이라고 이해했습니다. 지금 당장은 gradient가 손해인 방향이더라도 골짜기를 탈출할 기회를 마련하는 것이죠. 결국 둘은 정반대의 행동이므로 Trade-off 관계라고 부릅니다. 강화학습을 공부하는 사람들은 이 둘의 밸런스 조절에 목표를 두죠.

가장 쉬운 방법 중 하나를 소개시켜드리면, Epsilon Greedy method 입니다. Epsilon 이라는 Threshold를 만들고, random value가 epsilon보다 낮으면 Exploration을, 아니면 Exploitation을 하는 것이죠. Validating 때는 Epsilon을 0으로 고정하면 되겠죠?

강화학습에서 학습 중에 일어날 수 있는 문제가 무엇인지, 그걸 해결하는 개념이 무엇인 지에 대한 이해를 원하는 면접관이라면 던질 수 있을 것 같아 보입니다!
