# Word2Vec

오늘은 임베딩 방법론 중 하나인 Word2Vec에 대해 알아봅시다! Word2Vec을 설명하기 전에 첫 임베딩 방법론인 만큼 "임베딩"에 대해 간단하게 짚고 넘어가 볼게요.

먼저 임베딩이란 무엇이고 왜 하는 걸까요? 최근 기계번역, 요약, 문장 생성 등 다양한 NLP Task가 주목 받고 있습니다. 이런 주목을 받는 것은 컴퓨터가 인간의 말을 알아듣고 사람처럼 글을 쓸 수 있다는 점이 큰 작용을 하겠죠! 하지만 모두 아시다시피 컴퓨터는 빠르고 효율적인 계산기 같은 존재입니다. 그럼 인간의 말 "자연어"를 이해시키려면 컴퓨터가 알아 들을 수 있는 숫자들로 변형을 해야합니다. 이런 과정을 위해 사람이 쓰는 "자연어"를 기계가 이해할 수 있는 숫자의 나열인 "벡터"로 바꾼 결과 혹은 그 과정을 "임베딩"이라고 합니다.

2017년 이전의 임베딩 기법들은 NPLM, Word2Vec, Glove, FastText, Swivel 등 대부분 "단어" 순준 모델이었으며, 2018년 초 ELMo가 발표된 이후 BERT, GPT같은 "문장" 수준 임베딩 기법이 주목을 받기 시작했습니다. 그 중에서 오늘은 "단어"수준 임베딩 기법중 하나인 Word2Vec에 대해 알아보도록 할게요!

Word2Vec은 널리 쓰이고 있는 단어 임베딩 모델 중 하나입니다. 이 모델을 제안한 논문 안에는 Skip-gram과 CBOW라는 모델, 그리고 네거티브 샘플링 등의 학습 최적화 기법을 제안한 내용들이 포함되어있죠.

CBOW는 주변에 있는 문맥 단어를 가지고 타깃 단어 하나를 맞추는 과정에서 학습됩니다. Skip-gram은 타깃 단어를 가지고 주변 문맥 단어가 무엇일지 예측 하는 과정에서 학습되죠. 아래 그림을 보면 CBOW의 경우 입력, 출력 학습 데이터 쌍이 {문맥 단어 4개, 타깃 단어} 하나인 반면, Skip-gram의 학습 데이터는 {타깃 단어, 타깃 직전 두번째 단어}, {타깃 단어, 타깃 직전 단어}, {타깃 단어, 타깃 다음 단어}, {타깃 단어, 타깃 다음 두번째 단어} 이렇게 4개 쌍이 됩니다. 

"The fat cat sat on the mat"이라는 문장으로 예를 들어봅시다. 가운데 단어를 예측하는 것이 CBOW라고 했죠. {"The", "fat", "cat", "on", "the", "mat"}으로부터 sat을 예측하는 것은 CBOW가 하는 일입니다. 중심 단어를 예측하기 위해서 앞, 뒤로 몇 개의 단어를 볼지를 결정했다면 이 범위를 윈도우라고 합니다. 예를 들어서 윈도우 크기가 2이고, 예측하고자 하는 중심 단어가 sat이라고 한다면 앞의 두 단어인 fat와 cat, 그리고 뒤의 두 단어인 on, the를 참고합니다. 윈도우 크기가 n이라고 한다면, 실제 중심 단어를 예측하기 위해 참고하려고 하는 주변 단어의 개수는 2n이 될 것입니다. Skip-gram인 경우에는 동일한 예문에서 윈도우 크기가 2일때 데이터 셋은 {cat, The}, {cat, Fat}, {cat, sat}, {cat, on}, {sat, fat}, {sat, cat}, {sat, on}, {sat, the}가 되겠죠.

이를 통해 Skip-gram이 같은 말뭉치로도 더 많은 학습 데이터를 확보할 수 있다는 것을 알 수 있습니다. 따라서 임베딩 품질이 CBOW보다 좋은 경향이 있습니다. 다음은 학습 과정을 살펴볼까요? 여기서는 CBOW를 예를 들어 설명해 볼게요. 수식까지 설명하긴 너무.. 빡세니 수식은 각자 찾아봅시다! 개념적으로 간략하게 갈게요!

학습시킬 문장의 모든 단어들을 one-hot encoding방식으로 벡터화합니다. 하나의 중심 단어에 대해 2m개의 단어 벡터를 Input으로 갖습니다. CBOW방식의 파라미터는 Input layer -> Hidden layer로 가는 weights(W)와 Hidden layer -> Output layer로 가는 weights(W)가 있습니다. 각 단어에 파라미터 W를 곱하면 각 단어들은 one-hot encoding 벡터이기 때문에 각 단어에 대응하는 W의 행이 나옵니다. 이 결과로 나온 값을 그 단어에 대응하는 embedding vector가 되겠죠. 그리고 각 단어마다 가지고 있는 embedding vector의 평균을 내고 거기에 파라미터 U를 곱해 score를 계산합니다. 마지막으로 소프트맥스 함수를 이용하여 확률값으로 만들어줍니다. 이제 이 모델로부터 최적의 output을 뽑아내기 위해 학습의 과정을 거쳐야 합니다. loss function을 줄이기 위해 SGD를 이용하여 학습을 한 후 나온 행렬 W에서 각 행을 각 단어의 embedding vector로 사용하게 됩니다. (위에서 말했듯이 one-hot encoding이기 때문이죠.)

오늘은 좀 길었네요! negative sampling, subsampling frequent words 등 Word2Vec의 기법(?)들은 나중에 다시 다뤄보도록 합시당!
